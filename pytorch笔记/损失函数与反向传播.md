### 1、损失函数：计算模型输出和目标值的差距，为我们更新输出提供一定的依据（反向传播）

![](https://github.com/WeiGuang1214/Study-Notes/blob/master/images/1727190568533.jpg)



例子：

```python
import torch
from torch import nn
from torch.nn import MaxPool2d
from torch.utils.tensorboard import SummaryWriter

dataset = torchvision.datasets.CIFAR10("../data",train=False,download=True,
                                       			           
                                  transform=torchvision.transforms.ToTensor())

dataloader = DataLoader(dataset,batch_size=64)

inputs = toprch.tensor([1,2,3])
targets = torch.tensor([1,2,5])

inputs = torch.reshape(inputs,(1,1,1,3))
targets = torch.reshape(tragets,(1,1,1,3))

lossL1 = L1loss()
result = lossL1(inputs,targets)

lossMSE = MSELoss()
result = lossMSE(inputs,targets)
print(result)
```

### MESLoss，均方误差，

### 交叉熵用于解决多分类问题

![](https://github.com/WeiGuang1214/Study-Notes/blob/master/images/1727190674350.jpg)

#### 梯度下降 gradient

![](https://github.com/WeiGuang1214/Study-Notes/blob/master/images/1727271352934.jpg)

#### backward函数是反向传播，会把损失函数对于每一个可以训练的参数进行求导，计算出梯度，然后result调用反向传播，会存在gradient里面作为梯度，优化器利用梯度下降法更新权重参数，使损失减少，这就是训练的过程

```py
lossMSE = MSELoss()
result = lossMSE(inputs,targets)
result.backward()   //反向传播
print(result)
```

