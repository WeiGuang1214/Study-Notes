### 1、正则化层 Normalization layers，加快网络训练速度

### 2、Recurrent Layers  

### 3、 Transformer层

### 4、dropout层

### 5、线性层Liner，类似于线性变换，K=x1+b，weight相当于k，bias相当于b

<img src="C:\Users\余鹏飞\AppData\Roaming\Typora\typora-user-images\image-20240901152958137.png" alt="image-20240901152958137" style="zoom:67%;" />



```python
import torchvision
from torch.utils.data import Dataloader

dataset = torchvision.dataset.CIFA10("../data",train-False,transform=torchvision.transforms.ToTensor(),download = True)
dataloader = Dataloader(dataset,batch_size=64)

class Tudui(nn.Module):
    def __init__(self):
        super(Tudui,self).__init__()
        self.linear1 = Linear(196608,10)
    def forward(self,input):
        output = self.linear1(input)
        return output
tudui = Tudui()
for data in dataloader:
    imgs,targets = data
    print(img.shape)
    output = torch.reshape(imgs,(1,1,1,-1))
    output = torch.flatten(imgs)//torch.flatten() //通道处理器，把输入展开成一维向量
    print(output.shape)
    output = tudui(output)
    print(output.shape)      //处理之后196608通道变成10
    
print(output)
```
